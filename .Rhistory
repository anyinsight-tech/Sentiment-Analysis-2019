{
require(plyr)
require(stringr)
list=lapply(sentences, function(sentence, pos.words, neg.words)
{
sentence = gsub('[[:punct:]]',' ',sentence)
sentence = gsub('[[:cntrl:]]','',sentence)
sentence = gsub('\\d+','',sentence)
sentence = gsub('\n','',sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
pp=sum(pos.matches)
nn = sum(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
list1=c(score, pp, nn)
return (list1)
}, pos.words, neg.words)
score_new=lapply(list, `[[`, 1)
pp1=score=lapply(list, `[[`, 2)
nn1=score=lapply(list, `[[`, 3)
scores.df = data.frame(score=score_new, text=sentences)
positive.df = data.frame(Positive=pp1, text=sentences)
negative.df = data.frame(Negative=nn1, text=sentences)
list_df=list(scores.df, positive.df, negative.df)
return(list_df)
}
#TABLE DATA
library(reshape)
sentimentAnalyser<-function(result)
{
#Creating a copy of result data frame
test1=result[[1]]
test2=result[[2]]
test3=result[[3]]
#Creating three different data frames for Score, Positive and Negative
#Removing text column from data frame
test1$text=NULL
test2$text=NULL
test3$text=NULL
#Storing the first row(Containing the sentiment scores) in variable q
q1=test1[1,]
q2=test2[1,]
q3=test3[1,]
qq1=melt(q1, var='Score')
qq2=melt(q2, var='Positive')
qq3=melt(q3, var='Negative')
qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL
#Creating data frame
table1 = data.frame(Text=result[[1]]$text, Score=qq1)
table2 = data.frame(Text=result[[2]]$text, Score=qq2)
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
#Merging three data frames into one
table_final=data.frame(Text=table1$Text, Positive=table2$value, Negative=table3$value, Score=table1$value)
#Storing Analysis in CSV file (Optional)
#write.csv(table_final,file="SentiAnalysis.csv",append = TRUE) #To store Analysis in CSV, uncomment this line
return(table_final)
}
percentage<-function(table_final)
{
#Positive Percentage
#Renaming
posSc=table_final$Positive
negSc=table_final$Negative
#Adding column
table_final$PosPercent = posSc/ (posSc+negSc)
#Replacing Nan with zero
pp = table_final$PosPercent
pp[is.nan(pp)] <- 0
table_final$PosPercent = pp*100
#Negative Percentage
#Adding column
table_final$NegPercent = negSc/ (posSc+negSc)
#Replacing Nan with zero
nn = table_final$NegPercent
nn[is.nan(nn)] <- 0
table_final$NegPercent = nn*100
write.csv(table_final,file="SentiAnalysis.csv",append = FALSE)
return(table_final)
}
wordDatabase()
twtList<-reactive({twtList<-searchTwitter(input$searchTerm, n=input$maxTweets, lang="en") })
tweets<-reactive({tweets<-TweetFrame(twtList() )})
result<-reactive({result<-score.sentiment(tweets(), pos.words, neg.words, .progress='none')})
table_final<-reactive({table_final<-sentimentAnalyser(  result() )})
table_final_percentage<-reactive({table_final_percentage<-percentage(  table_final() )})
output$tabledata<-renderTable(table_final_percentage())
#WORDCLOUD
wordclouds<-function(text)
{
library(tm)
library(wordcloud)
corpus <- Corpus(VectorSource(text))
#clean text
clean_text <- tm_map(corpus, removePunctuation)
#clean_text <- tm_map(clean_text, content_transformation)
clean_text <- tm_map(clean_text, content_transformer(tolower))
clean_text <- tm_map(clean_text, removeWords, stopwords("english"))
clean_text <- tm_map(clean_text, removeNumbers)
clean_text <- tm_map(clean_text, stripWhitespace)
return (clean_text)
}
text_word<-reactive({text_word<-wordclouds( tweets() )})
output$word <- renderPlot({ wordcloud(text_word(),random.order=F,max.words=80, col=rainbow(100), main="WordCloud", scale=c(4.5, 1)) })
#HISTOGRAM
output$histPos<- renderPlot({ hist(table_final()$Positive, col=rainbow(10), main="Histogram of Positive Sentiment", xlab = "Positive Score") })
output$histNeg<- renderPlot({ hist(table_final()$Negative, col=rainbow(10), main="Histogram of Negative Sentiment", xlab = "Negative Score") })
output$histScore<- renderPlot({ hist(table_final()$Score, col=rainbow(10), main="Histogram of Score Sentiment", xlab = "Overall Score") })
#Pie
slices <- reactive ({ slices <- c(sum(table_final()$Positive), sum(table_final()$Negative)) })
labels <- c("Positive", "Negative")
library(plotrix)
output$piechart <- renderPlot({ pie3D(slices(), labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis") })
#Top trending topics by
toptrends <- function(place)
{
a_trends = availableTrendLocations()
woeid = a_trends[which(a_trends$name==place),3]
trend = getTrends(woeid)
trends = trend[1:2]
dat <- cbind(trends$name)
dat2 <- unlist(strsplit(dat, split=", "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
return (dat4)
}
trend_table<-reactive({ trend_table<-toptrends(input$trendingTable) })
output$trendtable <- renderTable(trend_table())
# Top 20 users who mentioned the hashtag
toptweeters<-function(tweetlist)
{
tweets <- twListToDF(tweetlist)
tweets <- unique(tweets)
# Make a table for the number of tweets per user
d <- as.data.frame(table(tweets$screenName))
d <- d[order(d$Freq, decreasing=T), ] #descending order of top charts according to frequency of tweets
names(d) <- c("User","Tweets")
return (d)
}
d<-reactive({d<-toptweeters(  twtList() ) })
output$tweetersplot<-renderPlot ( barplot(head(d()$Tweets, 20), names=head(d()$User, 20), horiz=F, las=2, main="Top 20 users associated with the Hashtag", col=1) )
output$tweeterstable<-renderTable(head(d(),20))
})
shinyUI(fluidPage(
headerPanel("Twitter Sentiment Analysis"),
textOutput("time"),
# Getting User Inputs
sidebarPanel(
textInput("searchTerm", "Enter data to be searched with '#'", "#"),
sliderInput("maxTweets","Number of recent tweets to use for analysis:",min=5,max=1000,value=500),
submitButton(text="Analyse")
),
mainPanel(
tabsetPanel(
tabPanel("Top Trending Topics Today",HTML("<div>Top Trending Topics worldwide or in selected countries</div>"),
selectInput("trendingTable","Choose the country",c("Worldwide","Germany", "Spain", "France"), selected = "Worldwide", selectize = FALSE),
submitButton(text="Search"),HTML("<div><h3> What's hot? </h3></div>"),
tableOutput("trendtable")),
tabPanel("Pie Chart",HTML("<div><h3>Pie Chart</h3></div>"), plotOutput("piechart"),HTML
("<div><h4> The pie chart presents the amount of positive and negative sentiments. Each side is proportional
to the quantity it represents.</h4></div>")),
tabPanel("Histogram",HTML
("<div><h3> The histograms depicts the sentiments on a score basis. A higher score indicates stronger
positive or negative sentiment in the tweet.</h3></div>"), plotOutput("histPos"), plotOutput("histNeg"), plotOutput("histScore")),
tabPanel("WordCloud",HTML("<div><h3>Most used words associated with the hashtag</h3></div>"),plotOutput("word"),
HTML
("<div><h4> The wordcloud demonstrates the attributes and traits related to the specific topic. It is
an effective and clear representation of the opinions towards the topic.</h4></div>")),
tabPanel("Table",HTML( "<div><h3> Storing the Tweets associated with the Hashtag in Tabular Format </h3></div>"), tableOutput("tabledata"),
HTML ("<div><h4> This table shows the original tweets and the sentiment scores given by the word-matching method. </h4></div>")),
tabPanel("Top Charts",HTML
("<div><h3> Top 20 users who used that Hashtag</h3></div>"),plotOutput("tweetersplot"), tableOutput("tweeterstable"))
)#end of tabset panel
)#end of main panel
))
runApp('GitHub/Sentiment-Analysis-2019')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
install.packages("plotrix")
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
lapply?
? lapply
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
runApp('prueba_proyecto_R')
# Load Requried Packages
library("SnowballC")
install.packages("Snowballc")
SnoballC
install.packages("SnowballC")
# Load Requried Packages
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
install.packages("syuzhet")
# Load Requried Packages
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
# Authonitical keys
consumer_key <- "lmqESukpqZ8qoMBhnkmGlYlGH"
consumer_secret <- "WkuhjMWc0yhM9QvDR7F2iHEwlgA1sfXSW8eknzKnfxiFCd3Kt1"
access_token <- "1013470531-svc9qeqfoRcnt5nFrIXjqi97Ke5z1qlR44k0buW"
access_secret <- "37jz2UZzBod3vA6GrZHAn4TmGDFI7bFTEYQZHfDNpekiM"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Cleaning the tweets for further analysis
tweets.df <- twListToDF(tweets)
# Load Requried Packages
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
# Authonitical keys
consumer_key <- "lmqESukpqZ8qoMBhnkmGlYlGH"
consumer_secret <- "WkuhjMWc0yhM9QvDR7F2iHEwlgA1sfXSW8eknzKnfxiFCd3Kt1"
access_token <- "1013470531-svc9qeqfoRcnt5nFrIXjqi97Ke5z1qlR44k0buW"
access_secret <- "37jz2UZzBod3vA6GrZHAn4TmGDFI7bFTEYQZHfDNpekiM"
# Load Requried Packages
library("SnowballC")
library("tm")
library("twitteR")
library("syuzhet")
# Authonitical keys
consumer_key <- "lmqESukpqZ8qoMBhnkmGlYlGH"
consumer_secret <- "WkuhjMWc0yhM9QvDR7F2iHEwlgA1sfXSW8eknzKnfxiFCd3Kt1"
access_token <- "1013470531-svc9qeqfoRcnt5nFrIXjqi97Ke5z1qlR44k0buW"
access_secret <- "37jz2UZzBod3vA6GrZHAn4TmGDFI7bFTEYQZHfDNpekiM"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets <- userTimeline("realDonaldTrump", n=200)
n.tweet <- length(tweets)
#Cleaning the tweets for further analysis
tweets.df <- twListToDF(tweets)
head(tweets.df)
tweets.df2 <- gsub("http.*","",tweets.df$text)
tweets.df2 <- gsub("https.*","",tweets.df2)
tweets.df2 <- gsub("#.*","",tweets.df2)
tweets.df2 <- gsub("@.*","",tweets.df2)
#mostrar
head(tweets.df2)
#Getting sentiment score for each tweet
word.df <- as.vector(tweets.df2)
emotion.df <- get_nrc_sentiment(word.df)
emotion.df2 <- cbind(tweets.df2, emotion.df)
head(emotion.df2)
sent.value <- get_sentiment(word.df)
most.positive <- word.df[sent.value == max(sent.value)]
most.positive
most.negative <- word.df[sent.value <= min(sent.value)]
most.negative
sent.value
################### Segregating positive and negative tweets
positive.tweets <- word.df[sent.value > 0]
head(positive.tweets)
negative.tweets <- word.df[sent.value < 0] >
head(negative.tweets)
### done, now group
table(category_senti)
category_senti
Negative Neutral Positive
install.packages("twitteR", repos = "http://cran.us.r-project.org")
install.packages("RCurl", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("syuzhet", repos = "http://cran.us.r-project.org")
install.packages("plotly")
install.packages("twitteR", repos = "http://cran.us.r-project.org")
install.packages("RCurl", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("syuzhet", repos = "http://cran.us.r-project.org")
install.packages("RCurl", repos = "http://cran.us.r-project.org")
install.packages("httr", repos = "http://cran.us.r-project.org")
install.packages("syuzhet", repos = "http://cran.us.r-project.org")
install.packages("syuzhet", repos = "http://cran.us.r-project.org")
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
#Merging three data frames into one
table_final=data.frame(Text=table1$Text, Positive=table2$value, Negative=table3$value, Score=table1$value)
shinyUI(fluidPage(
headerPanel("Twitter Sentiment Analysis"),
textOutput("time"),
# Getting User Inputs
sidebarPanel(
textInput("searchTerm", "Enter data to be searched with '#'", "#"),
sliderInput("maxTweets","Number of recent tweets to use for analysis:",min=5,max=1000,value=500),
submitButton(text="Analyse")
),
mainPanel(
tabsetPanel(
tabPanel("Top Trending Topics Today",HTML("<div>Top Trending Topics worldwide or in selected countries</div>"),
selectInput("trendingTable","Choose the country",c("Worldwide","Germany", "Spain", "France"), selected = "Worldwide", selectize = FALSE),
submitButton(text="Search"),HTML("<div><h3> What's hot? </h3></div>"),
tableOutput("trendtable")),
tabPanel("Pie Chart",HTML("<div><h3>Pie Chart</h3></div>"), plotOutput("piechart"),HTML
("<div><h4> The pie chart presents the amount of positive and negative sentiments. Each side is proportional
to the quantity it represents.</h4></div>")),
tabPanel("Histogram",HTML
("<div><h3> The histograms depicts the sentiments on a score basis. A higher score indicates stronger
positive or negative sentiment in the tweet.</h3></div>"), plotOutput("histPos"), plotOutput("histNeg"), plotOutput("histScore")),
tabPanel("WordCloud",HTML("<div><h3>Most used words associated with the hashtag</h3></div>"),plotOutput("word"),
HTML
("<div><h4> The wordcloud demonstrates the attributes and traits related to the specific topic. It is
an effective and clear representation of the opinions towards the topic.</h4></div>")),
tabPanel("Table",HTML( "<div><h3> Storing the Tweets associated with the Hashtag in Tabular Format </h3></div>"), tableOutput("tabledata"),
HTML ("<div><h4> This table shows the original tweets and the sentiment scores given by the word-matching method. </h4></div>")),
tabPanel("Top Charts",HTML
("<div><h3> Top 20 users who used that Hashtag</h3></div>"),plotOutput("tweetersplot"), tableOutput("tweeterstable"))
)#end of tabset panel
)#end of main panel
))
library(shiny)
PrepareTwitter<-function()
{
library(twitteR)
library(stringr)
library(ROAuth)
library(RCurl)
library(ggplot2)
library(reshape)
library(tm)
library(RJSONIO)
library(wordcloud)
library(gridExtra)
library(plyr)
library(e1071)
}
PrepareTwitter()
shinyServer(function(input, output, session) {
consumer_key <- "lmqESukpqZ8qoMBhnkmGlYlGH"
consumer_secret <- "WkuhjMWc0yhM9QvDR7F2iHEwlgA1sfXSW8eknzKnfxiFCd3Kt1"
access_token <- "1013470531-svc9qeqfoRcnt5nFrIXjqi97Ke5z1qlR44k0buW"
access_secret <- "37jz2UZzBod3vA6GrZHAn4TmGDFI7bFTEYQZHfDNpekiM"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
# Clean the tweets
TweetFrame<-function(twtList)
{df<- do.call("rbind",lapply(twtList,as.data.frame))
#    df$text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
#    df$text = gsub("(f|ht)tp(s?)://(.*)[.][a-z]+", "", df$text)
return (df$text)
}
tweets <- userTimeline("realDonaldTrump", n=200)
n.tweet <- length(tweets)
# Store lexicons in vectors
pos.words = scan("https://raw.githubusercontent.com/isabellashao/Sentiment-Analysis-2019/master/positive-words.txt?token=ALMIODQUIEOIMGYYD4LDWJ244QFF6", what='character', comment.char=';')
neg.words = scan("https://raw.githubusercontent.com/isabellashao/Sentiment-Analysis-2019/master/negative-words.txt?token=ALMIODRF5ACXBM6WFI2IXKC44QE2K", what='character', comment.char=';')
wordDatabase<-function()
{
pos.words<<-c(pos.words)
neg.words<<-c(neg.words)
}
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
list=lapply(sentences, function(sentence, pos.words, neg.words)
{
sentence = gsub('[[:punct:]]',' ',sentence)
sentence = gsub('[[:cntrl:]]','',sentence)
sentence = gsub('\\d+','',sentence)
sentence = gsub('\n','',sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
pp=sum(pos.matches)
nn = sum(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
list1=c(score, pp, nn)
return (list1)
}, pos.words, neg.words)
score_new=lapply(list, `[[`, 1)
pp1=score=lapply(list, `[[`, 2)
nn1=score=lapply(list, `[[`, 3)
scores.df = data.frame(score=score_new, text=sentences)
positive.df = data.frame(Positive=pp1, text=sentences)
negative.df = data.frame(Negative=nn1, text=sentences)
list_df=list(scores.df, positive.df, negative.df)
return(list_df)
}
#TABLE DATA
library(reshape)
sentimentAnalyser<-function(result)
{
#Creating a copy of result data frame
test1=result[[1]]
test2=result[[2]]
test3=result[[3]]
#Creating three different data frames for Score, Positive and Negative
#Removing text column from data frame
test1$text=NULL
test2$text=NULL
test3$text=NULL
#Storing the first row(Containing the sentiment scores) in variable q
q1=test1[1,]
q2=test2[1,]
q3=test3[1,]
qq1=melt(q1, var='Score')
qq2=melt(q2, var='Positive')
qq3=melt(q3, var='Negative')
qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL
#Creating data frame
table1 = data.frame(Text=result[[1]]$text, Score=qq1)
table2 = data.frame(Text=result[[2]]$text, Score=qq2)
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
#Merging three data frames into one
table_final=data.frame(Text=table1$Text, Positive=table2$value, Negative=table3$value, Score=table1$value)
#Storing Analysis in CSV file (Optional)
#write.csv(table_final,file="SentiAnalysis.csv",append = TRUE) #To store Analysis in CSV, uncomment this line
return(table_final)
}
percentage<-function(table_final)
{
#Positive Percentage
#Renaming
posSc=table_final$Positive
negSc=table_final$Negative
#Adding column
table_final$PosPercent = posSc/ (posSc+negSc)
#Replacing Nan with zero
pp = table_final$PosPercent
pp[is.nan(pp)] <- 0
table_final$PosPercent = pp*100
#Negative Percentage
#Adding column
table_final$NegPercent = negSc/ (posSc+negSc)
#Replacing Nan with zero
nn = table_final$NegPercent
nn[is.nan(nn)] <- 0
table_final$NegPercent = nn*100
write.csv(table_final,file="SentiAnalysis.csv",append = FALSE)
return(table_final)
}
wordDatabase()
twtList<-reactive({twtList<-searchTwitter(input$searchTerm, n=input$maxTweets, lang="en") })
tweets<-reactive({tweets<-TweetFrame(twtList() )})
result<-reactive({result<-score.sentiment(tweets(), pos.words, neg.words, .progress='none')})
table_final<-reactive({table_final<-sentimentAnalyser(  result() )})
table_final_percentage<-reactive({table_final_percentage<-percentage(  table_final() )})
output$tabledata<-renderTable(table_final_percentage())
#WORDCLOUD
wordclouds<-function(text)
{
library(tm)
library(wordcloud)
corpus <- Corpus(VectorSource(text))
#clean text
clean_text <- tm_map(corpus, removePunctuation)
#clean_text <- tm_map(clean_text, content_transformation)
clean_text <- tm_map(clean_text, content_transformer(tolower))
clean_text <- tm_map(clean_text, removeWords, stopwords("english"))
clean_text <- tm_map(clean_text, removeNumbers)
clean_text <- tm_map(clean_text, stripWhitespace)
return (clean_text)
}
text_word<-reactive({text_word<-wordclouds( tweets() )})
output$word <- renderPlot({ wordcloud(text_word(),random.order=F,max.words=80, col=rainbow(100), main="WordCloud", scale=c(4.5, 1)) })
#HISTOGRAM
output$histPos<- renderPlot({ hist(table_final()$Positive, col=rainbow(10), main="Histogram of Positive Sentiment", xlab = "Positive Score") })
output$histNeg<- renderPlot({ hist(table_final()$Negative, col=rainbow(10), main="Histogram of Negative Sentiment", xlab = "Negative Score") })
output$histScore<- renderPlot({ hist(table_final()$Score, col=rainbow(10), main="Histogram of Score Sentiment", xlab = "Overall Score") })
#Pie
slices <- reactive ({ slices <- c(sum(table_final()$Positive), sum(table_final()$Negative)) })
labels <- c("Positive", "Negative")
library(plotrix)
output$piechart <- renderPlot({ pie3D(slices(), labels = labels, col=rainbow(length(labels)),explode=0.00, main="Sentiment Analysis") })
#Top trending topics by
toptrends <- function(place)
{
a_trends = availableTrendLocations()
woeid = a_trends[which(a_trends$name==place),3]
trend = getTrends(woeid)
trends = trend[1:2]
dat <- cbind(trends$name)
dat2 <- unlist(strsplit(dat, split=", "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
return (dat4)
}
trend_table<-reactive({ trend_table<-toptrends(input$trendingTable) })
output$trendtable <- renderTable(trend_table())
# Top 20 users who mentioned the hashtag
toptweeters<-function(tweetlist)
{
tweets <- twListToDF(tweetlist)
tweets <- unique(tweets)
# Make a table for the number of tweets per user
d <- as.data.frame(table(tweets$screenName))
d <- d[order(d$Freq, decreasing=T), ] #descending order of top charts according to frequency of tweets
names(d) <- c("User","Tweets")
return (d)
}
d<-reactive({d<-toptweeters(  twtList() ) })
output$tweetersplot<-renderPlot ( barplot(head(d()$Tweets, 20), names=head(d()$User, 20), horiz=F, las=2, main="Top 20 users associated with the Hashtag", col=1) )
output$tweeterstable<-renderTable(head(d(),20))
})
shiny::runApp('GitHub/Sentiment-Analysis-2019')
